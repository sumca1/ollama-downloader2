name: Download Ollama Models

on:
  workflow_dispatch:
    inputs:
      model_name:
        description: 'Model to download'
        required: true
        default: 'llama3.1-70b'
        type: choice
        options:
          - llama3.1-70b
          - llama3.1-8b
          - mixtral-8x7b
          - llava-13b
          - codellama-34b
          - qwen2.5-72b
          - mistral-7b
          - deepseek-coder-33b

jobs:
  download-and-release:
    runs-on: ubuntu-latest
    
    steps:
      - name: ðŸ“¦ Checkout repository
        uses: actions/checkout@v3
      
      - name: ðŸ” Set model details
        id: model_info
        run: |
          case "${{ github.event.inputs.model_name }}" in
            llama3.1-70b)
              echo "url=https://huggingface.co/bartowski/Meta-Llama-3.1-70B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf" >> $GITHUB_OUTPUT
              echo "filename=llama3.1-70b-q4.gguf" >> $GITHUB_OUTPUT
              echo "size=40GB" >> $GITHUB_OUTPUT
              echo "description=×”×›×™ ×—×–×§ - ×ž×¦×•×™×Ÿ ×œ×¢×‘×¨×™×ª ×•×ž×©×™×ž×•×ª ×ž×•×¨×›×‘×•×ª" >> $GITHUB_OUTPUT
              ;;
            llama3.1-8b)
              echo "url=https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf" >> $GITHUB_OUTPUT
              echo "filename=llama3.1-8b-q4.gguf" >> $GITHUB_OUTPUT
              echo "size=4.9GB" >> $GITHUB_OUTPUT
              echo "description=×ž××•×–×Ÿ - ×˜×•×‘ ×œ×¢×‘×¨×™×ª, ×ž×”×™×¨" >> $GITHUB_OUTPUT
              ;;
            mixtral-8x7b)
              echo "url=https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf" >> $GITHUB_OUTPUT
              echo "filename=mixtral-8x7b-q4.gguf" >> $GITHUB_OUTPUT
              echo "size=26GB" >> $GITHUB_OUTPUT
              echo="description=×ž×”×™×¨ ×•××™×›×•×ª×™ - 8 ×ž×•×ž×—×™× ×‘×ž×§×‘×™×œ" >> $GITHUB_OUTPUT
              ;;
            llava-13b)
              echo "url=https://huggingface.co/mys/ggml_llava-v1.5-13b/resolve/main/ggml-model-q4_k.gguf" >> $GITHUB_OUTPUT
              echo "mmproj_url=https://huggingface.co/mys/ggml_llava-v1.5-13b/resolve/main/mmproj-model-f16.gguf" >> $GITHUB_OUTPUT
              echo "filename=llava-13b-q4.gguf" >> $GITHUB_OUTPUT
              echo "mmproj_filename=llava-13b-mmproj.gguf" >> $GITHUB_OUTPUT
              echo "size=8GB+2GB" >> $GITHUB_OUTPUT
              echo "description=×¨××™×™×” ×ž×ž×•×—×©×‘×ª - OCR, ×ª×™××•×¨ ×ª×ž×•× ×•×ª" >> $GITHUB_OUTPUT
              ;;
            codellama-34b)
              echo "url=https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q4_K_M.gguf" >> $GITHUB_OUTPUT
              echo "filename=codellama-34b-q4.gguf" >> $GITHUB_OUTPUT
              echo "size=20GB" >> $GITHUB_OUTPUT
              echo "description=×ª×›× ×•×ª ×ž×ª×§×“× - ×§×•×“, debug, refactoring" >> $GITHUB_OUTPUT
              ;;
            qwen2.5-72b)
              echo "url=https://huggingface.co/bartowski/Qwen2.5-72B-Instruct-GGUF/resolve/main/Qwen2.5-72B-Instruct-Q4_K_M.gguf" >> $GITHUB_OUTPUT
              echo "filename=qwen2.5-72b-q4.gguf" >> $GITHUB_OUTPUT
              echo "size=42GB" >> $GITHUB_OUTPUT
              echo "description=×¡×™× ×™ ×ž×ª×§×“× - ×¨×‘ ×œ×©×•× ×™, ×—×–×§ ×ž××•×“" >> $GITHUB_OUTPUT
              ;;
            mistral-7b)
              echo "url=https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf" >> $GITHUB_OUTPUT
              echo "filename=mistral-7b-q4.gguf" >> $GITHUB_OUTPUT
              echo "size=4.1GB" >> $GITHUB_OUTPUT
              echo "description=×§×œ ×•×ž×”×™×¨ - ×˜×•×‘ ×œ×”×ª×—×œ×”" >> $GITHUB_OUTPUT
              ;;
            deepseek-coder-33b)
              echo "url=https://huggingface.co/TheBloke/deepseek-coder-33B-instruct-GGUF/resolve/main/deepseek-coder-33b-instruct.Q4_K_M.gguf" >> $GITHUB_OUTPUT
              echo "filename=deepseek-coder-33b-q4.gguf" >> $GITHUB_OUTPUT
              echo "size=19GB" >> $GITHUB_OUTPUT
              echo "description=×§×•×“ ×ž×ª×§×“× - ×ž×ª×—×¨×” ×‘-GPT-4 ×‘×ª×›× ×•×ª" >> $GITHUB_OUTPUT
              ;;
          esac
      
      - name: ðŸ“¥ Download model
        run: |
          echo "Downloading ${{ github.event.inputs.model_name }} from HuggingFace..."
          echo "Size: ${{ steps.model_info.outputs.size }}"
          echo "Description: ${{ steps.model_info.outputs.description }}"
          
          wget -O "${{ steps.model_info.outputs.filename }}" "${{ steps.model_info.outputs.url }}" || \
          curl -L -o "${{ steps.model_info.outputs.filename }}" "${{ steps.model_info.outputs.url }}"
          
          # ×”×•×¨×“×ª mmproj ×¢×‘×•×¨ LLaVA (×× ×§×™×™×)
          if [ ! -z "${{ steps.model_info.outputs.mmproj_url }}" ]; then
            echo "Downloading vision projector..."
            wget -O "${{ steps.model_info.outputs.mmproj_filename }}" "${{ steps.model_info.outputs.mmproj_url }}" || \
            curl -L -o "${{ steps.model_info.outputs.mmproj_filename }}" "${{ steps.model_info.outputs.mmproj_url }}"
          fi
          
          echo "Download complete!"
          ls -lh *.gguf
      
      - name: ðŸ“Š Generate SHA256 checksum
        run: |
          sha256sum "${{ steps.model_info.outputs.filename }}" > "${{ steps.model_info.outputs.filename }}.sha256"
          cat "${{ steps.model_info.outputs.filename }}.sha256"
      
      - name: ðŸ“¦ Download Ollama binary
        run: |
          echo "Downloading Ollama for Windows..."
          wget -O ollama-windows.exe https://github.com/ollama/ollama/releases/latest/download/ollama-windows-amd64.exe || \
          curl -L -o ollama-windows.exe https://github.com/ollama/ollama/releases/latest/download/ollama-windows-amd64.exe
          
          ls -lh ollama-windows.exe
      
      - name: ðŸ“ Create README for release
        run: |
          cat > RELEASE_README.md << 'EOF'
          # Ollama Model: ${{ github.event.inputs.model_name }}
          
          ## ðŸ“¦ ×§×‘×¦×™× ×œ×”×•×¨×“×”
          
          - **${{ steps.model_info.outputs.filename }}** - ×”×ž×•×“×œ (${{ steps.model_info.outputs.size }})
          - **${{ steps.model_info.outputs.filename }}.sha256** - Checksum ×œ××™×ž×•×ª
          - **ollama-windows.exe** - Ollama ×œWindows
          
          ## ðŸš€ ×”×•×¨××•×ª ×”×ª×§× ×”
          
          ### ×©×œ×‘ 1: ×”×•×¨×“ ××ª ×”×§×‘×¦×™×
          ```powershell
          # ×¦×•×¨ ×ª×™×§×™×™×”
          New-Item -ItemType Directory -Path "$HOME\ollama" -Force
          cd "$HOME\ollama"
          
          # ×”×•×¨×“ ××ª ×›×œ ×”×§×‘×¦×™× ×ž-Assets ×œ×ž×˜×”
          ```
          
          ### ×©×œ×‘ 2: ××ž×ª ××ª ×”×§×•×‘×¥ (××•×¤×¦×™×•× ×œ×™)
          ```powershell
          # ×—×©×‘ SHA256
          Get-FileHash "${{ steps.model_info.outputs.filename }}" -Algorithm SHA256
          
          # ×”×©×•×•×” ×œ×§×•×‘×¥ .sha256
          Get-Content "${{ steps.model_info.outputs.filename }}.sha256"
          ```
          
          ### ×©×œ×‘ 3: ×”×¤×¢×œ ××ª Ollama
          ```powershell
          # ×”×¤×¢×œ ××ª Ollama
          .\ollama-windows.exe serve
          ```
          
          ### ×©×œ×‘ 4: ×˜×¢×Ÿ ××ª ×”×ž×•×“×œ (×‘×—×œ×•×Ÿ × ×¤×¨×“)
          ```powershell
          # ×”×¢×ª×§ ××ª ×”×ž×•×“×œ ×œ×ª×™×§×™×™×ª Ollama
          $ollamaDir = "$env:USERPROFILE\.ollama\models"
          New-Item -ItemType Directory -Path $ollamaDir -Force
          Copy-Item "${{ steps.model_info.outputs.filename }}" "$ollamaDir\"
          
          # ×”×¨×¥ ××ª ×”×ž×•×“×œ
          .\ollama-windows.exe run ${{ github.event.inputs.model_name }}
          ```
          
          ## ðŸ’¡ ×‘×“×™×§×” ×ž×”×™×¨×”
          ```powershell
          # ×©××œ ×©××œ×”
          .\ollama-windows.exe run ${{ github.event.inputs.model_name }} "×ž×” ×–×” Python?"
          ```
          
          ## ðŸŒ ×©×™×ž×•×© ×ž-API
          ```python
          import requests
          
          response = requests.post('http://localhost:11434/api/generate',
              json={
                  'model': '${{ github.event.inputs.model_name }}',
                  'prompt': '×©×œ×•× ×¢×•×œ×!',
                  'stream': False
              })
          print(response.json()['response'])
          ```
          
          ---
          
          **×”×•×¨×“×” ××•×˜×•×ž×˜×™×ª ×‘-GitHub Actions**  
          × ×•×¦×¨: $(date)  
          ×’×•×“×œ: ${{ steps.model_info.outputs.size }}
          EOF
      
      - name: ðŸŽ‰ Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          tag_name: ${{ github.event.inputs.model_name }}-${{ github.run_number }}
          name: Ollama Model - ${{ github.event.inputs.model_name }} (Run ${{ github.run_number }})
          body_path: RELEASE_README.md
          files: |
            *.gguf
            *.sha256
            ollama-windows.exe
          draft: false
          prerelease: false
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      
      - name: âœ… Success notification
        run: |
          echo "âœ… ×”×•×¨×“×” ×”×•×©×œ×ž×”!"
          echo "ðŸ“¦ ×§×•×‘×¥: ${{ steps.model_info.outputs.filename }}"
          echo "ðŸ“Š ×’×•×“×œ: ${{ steps.model_info.outputs.size }}"
          echo "ðŸ”— ×”×ž×•×“×œ ×–×ž×™×Ÿ ×‘-Releases"
